{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_7400\\3362167292.py:13: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  X_train = np.array([np.array(image[0]) for image in train_dataset])\n",
      "C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_7400\\3362167292.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y_train = np.array(train_dataset.targets)\n",
      "C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_7400\\3362167292.py:15: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  X_test = np.array([np.array(image[0]) for image in test_dataset])\n",
      "C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_7400\\3362167292.py:16: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y_test = np.array(test_dataset.targets)\n"
     ]
    }
   ],
   "source": [
    "# loading MNIST dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "# preparing training and test data for Random Forest\n",
    "X_train = np.array([np.array(image[0]) for image in train_dataset])\n",
    "y_train = np.array(train_dataset.targets)\n",
    "X_test = np.array([np.array(image[0]) for image in test_dataset])\n",
    "y_test = np.array(test_dataset.targets)\n",
    "\n",
    "\n",
    "# preparing training and test data for ANN, CNN\n",
    "train_len = int(len(train_dataset) * 0.8)\n",
    "valid_len = int(len(train_dataset) - train_len)\n",
    "\n",
    "train_ds, valid_ds = random_split(train_dataset, [train_len, valid_len])\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dl = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining abstract class using interface\n",
    "class MnistClassifierInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def train(self, X_train, y_train):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, X_test):\n",
    "        pass\n",
    "    \n",
    "class TrainPredictAbstract(MnistClassifierInterface):\n",
    "    def __init__(self, model,  num_epochs=10, lr=0.0001, device='cpu'):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.model = model.to(device) # initializing out model\n",
    "        self.loss_fn = nn.CrossEntropyLoss() # CrossEntropyLoss is a suitable loss function in our case, as it designed for multi-class classification\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr) # setting up optimizer to update model parameters\n",
    "        \n",
    "    def train(self, train_dl, valid_dl):\n",
    "        \n",
    "        scaler = GradScaler(device=self.device) # initializing GradScaler for mixed precisions(using float16 where possible) to reduce memory usage, faster computations(works only with GPU). \n",
    "        torch.backends.cudnn.benchmark = True # by setting benchmark to True PyTorch will find the fastest algorithm on operations like convolutions for your hardware\n",
    "        \n",
    "        writer = SummaryWriter() # initializing SummaryWriter instance to enable tensorboard to track train and validation losses and accuracies\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train() # setting model to train phase\n",
    "            \n",
    "            # variables to track model performance on training data\n",
    "            total_samples_train = 0\n",
    "            total_loss_train = 0\n",
    "            total_correct_train = 0\n",
    "            \n",
    "            for x_batch, y_batch in train_dl:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                self.optimizer.zero_grad() # clearing gradients to avoid their summation\n",
    "\n",
    "                with autocast(device_type=self.device): # enabling mixed precisions\n",
    "                    pred = self.model(x_batch)\n",
    "                    loss = self.loss_fn(pred, y_batch)\n",
    "\n",
    "                scaler.scale(loss).backward() # scaling loss before backpropagation serves stable performance by preventing gradient underflow in float16\n",
    "                scaler.step(self.optimizer) # unscales gradients \n",
    "                scaler.update() # update weights\n",
    "\n",
    "                total_loss_train += loss.item() * y_batch.size(0)\n",
    "                total_correct_train += (torch.argmax(pred, dim=1) == y_batch).sum().item()\n",
    "                total_samples_train += y_batch.size(0)\n",
    "        \n",
    "            train_loss = total_loss_train / len(train_dl.dataset)\n",
    "            train_accuracy = total_correct_train / total_samples_train\n",
    "\n",
    "\n",
    "            self.model.eval() # setting model to evaluation phase\n",
    "\n",
    "            # variables to track model performance on validation data\n",
    "            total_correct_valid = 0\n",
    "            total_samples_valid = 0\n",
    "            total_loss_valid = 0\n",
    "\n",
    "            with torch.inference_mode(): # disabling gradients calculation to measure the model performance during epoch\n",
    "                for x_batch, y_batch in valid_dl:\n",
    "                    x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "                    with autocast(device_type=self.device):\n",
    "                        pred = self.model(x_batch)\n",
    "                        loss = self.loss_fn(pred, y_batch)\n",
    "\n",
    "                    total_loss_valid += loss.item() * y_batch.size(0)\n",
    "                    total_correct_valid += (torch.argmax(pred, dim=1) == y_batch).sum().item()\n",
    "                    total_samples_valid += y_batch.size(0)\n",
    "\n",
    "            valid_loss = total_loss_valid / len(valid_dl.dataset)\n",
    "            valid_accuracy = total_correct_valid / total_samples_valid\n",
    "\n",
    "\n",
    "            # writing losses and accuracies to folders to track them in tensorboard\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "            writer.add_scalar(\"Loss/Validation\", valid_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Valid\", valid_accuracy, epoch)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}, train acc: {train_accuracy:.4f}, valid acc: {valid_accuracy:.4f}')\n",
    "\n",
    "        writer.close()\n",
    "            \n",
    "    def predict(self, test_dl):\n",
    "        self.model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.inference_mode():\n",
    "            for x_batch, y_batch in test_dl:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                pred = self.model(x_batch)\n",
    "                \n",
    "                total_correct += (torch.argmax(pred, dim=1) == y_batch).sum().item()\n",
    "                total_samples += y_batch.size(0)\n",
    "                \n",
    "        return (total_correct / total_samples)\n",
    "    \n",
    "# Random Forest implementation   \n",
    "class RandomForestMnistClassifier(MnistClassifierInterface):\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestClassifier()\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        X_train_flattened = X_train.reshape(X_train.shape[0], -1) # flatten to receive 1d array, as the model train with 2d array\n",
    "        self.model.fit(X_train_flattened, y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        X_test_flattened = X_test.reshape(X_test.shape[0], -1) # flatten to receive 1d array, as the model predict with 2d array\n",
    "        return self.model.predict(X_test_flattened)\n",
    "\n",
    "# Feed-Forward Neural Network      \n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=64, output_dim=10):\n",
    "        super(FFNN, self).__init__() # call parent constructor to initialize arguments\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    # forward method to pass input data thought fully connected layers and introduce non-linearity by activation function ReLU    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class FFNNMnistClassifier(TrainPredictAbstract):\n",
    "   def __init__(self, num_epochs=10, lr=0.0001, device='cpu'):\n",
    "       model = FFNN()\n",
    "       super().__init__(model=model, num_epochs=num_epochs, lr=lr, device=device)\n",
    "    \n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_dim=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # input_channel = 1 because image is grayscale, [batch(64), 32, 28, 28]\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32) # by normalizing layers inputs we make training faster and more stable\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2) # [batch, 32, 14, 14]\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #  [batch, 64, 14, 14]\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64) \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2) # [batch, 64, 7, 7]\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.4) # disable some neurons while training to reduce overfitting\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.batchnorm1(self.conv1(x)))) # stack methods to reduce space \n",
    "        \n",
    "        x = self.pool2(F.relu(self.batchnorm2(self.conv2(x))))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        return self.fc2(x)\n",
    "    \n",
    "class CNNMnistClassifier(TrainPredictAbstract):\n",
    "    def __init__(self, num_epochs=10, lr=0.0001, device='cpu'):\n",
    "        model = CNN()\n",
    "        super().__init__(model=model, num_epochs=num_epochs, lr=lr, device=device)\n",
    "        \n",
    "        \n",
    "class MnistClassifier:\n",
    "    def __init__(self, algorithm='rf', **kwargs):\n",
    "        if algorithm == 'rf':\n",
    "            self.model = RandomForestMnistClassifier(**kwargs)\n",
    "        elif algorithm == 'nn':\n",
    "            self.model = FFNNMnistClassifier(**kwargs)\n",
    "        elif algorithm == 'cnn':\n",
    "            self.model = CNNMnistClassifier(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect alogrithm, pick one of those: 'cnn', 'nn', 'rf'\")\n",
    "    \n",
    "    def train(self, X_train=None, y_train=None, train_dl=None, valid_dl=None):\n",
    "        if isinstance(self.model, RandomForestMnistClassifier):\n",
    "            self.model.train(X_train, y_train)\n",
    "        else:\n",
    "            self.model.train(train_dl, valid_dl)\n",
    "            \n",
    "    def predict(self, X_test=None, test_dl=None):\n",
    "        if isinstance(self.model, RandomForestMnistClassifier):\n",
    "            return self.model.predict(X_test)\n",
    "        else:\n",
    "            return self.model.predict(test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = MnistClassifier('rf')\n",
    "rf.train(X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "pred = rf.predict(X_test)\n",
    "print(f\"Random forest accuracy: {(pred == y_test).sum() / y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train acc: 0.8970, valid acc: 0.9712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m cnn \u001b[38;5;241m=\u001b[39m MnistClassifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[96], line 182\u001b[0m, in \u001b[0;36mMnistClassifier.train\u001b[1;34m(self, X_train, y_train, train_dl, valid_dl)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[96], line 43\u001b[0m, in \u001b[0;36mTrainPredictAbstract.train\u001b[1;34m(self, train_dl, valid_dl)\u001b[0m\n\u001b[0;32m     40\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_batch)\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(pred, y_batch)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# scaling loss before backpropagation serves stable performance by preventing gradient underflow in float16\u001b[39;00m\n\u001b[0;32m     44\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;66;03m# unscales gradients \u001b[39;00m\n\u001b[0;32m     45\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate() \u001b[38;5;66;03m# update weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrii\\anaconda3\\envs\\project1\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrii\\anaconda3\\envs\\project1\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrii\\anaconda3\\envs\\project1\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn = MnistClassifier('cnn', num_epochs=5)\n",
    "cnn.train(train_dl=train_dl, valid_dl=valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
