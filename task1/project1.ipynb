{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:04:05.716022Z","iopub.status.busy":"2025-02-25T10:04:05.715715Z","iopub.status.idle":"2025-02-25T10:04:26.335398Z","shell.execute_reply":"2025-02-25T10:04:26.334635Z","shell.execute_reply.started":"2025-02-25T10:04:05.716001Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","from torchvision import datasets, transforms\n","from abc import ABC, abstractmethod\n","from sklearn.ensemble import RandomForestClassifier\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.amp import autocast, GradScaler\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:04:26.336812Z","iopub.status.busy":"2025-02-25T10:04:26.336346Z","iopub.status.idle":"2025-02-25T10:04:38.624683Z","shell.execute_reply":"2025-02-25T10:04:38.623902Z","shell.execute_reply.started":"2025-02-25T10:04:26.336791Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_13172\\275371216.py:13: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n","  X_train = np.array([np.array(image[0]) for image in train_dataset])\n","C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_13172\\275371216.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n","  y_train = np.array(train_dataset.targets)\n","C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_13172\\275371216.py:15: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n","  X_test = np.array([np.array(image[0]) for image in test_dataset])\n","C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_13172\\275371216.py:16: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n","  y_test = np.array(test_dataset.targets)\n"]}],"source":["# loading MNIST dataset\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","train_dataset = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n","\n","test_dataset = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n","\n","\n","# preparing training and test data for Random Forest\n","X_train = np.array([np.array(image[0]) for image in train_dataset])\n","y_train = np.array(train_dataset.targets)\n","X_test = np.array([np.array(image[0]) for image in test_dataset])\n","y_test = np.array(test_dataset.targets)\n","\n","\n","# preparing training and test data for FFNN, CNN\n","train_len = int(len(train_dataset) * 0.8)\n","valid_len = int(len(train_dataset) - train_len)\n","\n","train_ds, valid_ds = random_split(train_dataset, [train_len, valid_len])\n","\n","train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n","\n","valid_dl = DataLoader(valid_ds, batch_size=64, shuffle=False)\n","\n","test_dl = DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:18:52.672786Z","iopub.status.busy":"2025-02-25T10:18:52.672384Z","iopub.status.idle":"2025-02-25T10:18:52.697233Z","shell.execute_reply":"2025-02-25T10:18:52.696364Z","shell.execute_reply.started":"2025-02-25T10:18:52.672758Z"},"trusted":true},"outputs":[],"source":["# Defining abstract class using interface\n","class MnistClassifierInterface(ABC):\n","    @abstractmethod\n","    def train(self, X_train, y_train):\n","        pass\n","    \n","    @abstractmethod\n","    def predict(self, X_test):\n","        pass\n","    \n","class TrainPredictAbstract(MnistClassifierInterface):\n","    def __init__(self, model,  num_epochs=10, lr=0.0001, device='cpu'):\n","        self.num_epochs = num_epochs\n","        self.lr = lr\n","        self.device = device\n","        self.model = model.to(device) # initializing out model\n","        self.loss_fn = nn.CrossEntropyLoss() # CrossEntropyLoss is a suitable loss function in our case, as it designed for multi-class classification\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr) # setting up optimizer to update model parameters\n","        \n","    def train(self, train_dl, valid_dl):\n","        \n","        scaler = GradScaler() # initializing GradScaler for mixed precisions(using float16 where possible) to reduce memory usage, faster computations(works only with GPU). \n","        torch.backends.cudnn.benchmark = True # by setting benchmark to True PyTorch will find the fastest algorithm on operations like convolutions for your hardware\n","        \n","        writer = SummaryWriter() # initializing SummaryWriter instance to enable tensorboard to track train and validation losses and accuracies\n","\n","        for epoch in range(self.num_epochs):\n","            self.model.train() # setting model to train phase\n","            \n","            # variables to track model performance on training data\n","            total_samples_train = 0\n","            total_loss_train = 0\n","            total_correct_train = 0\n","            \n","            for x_batch, y_batch in train_dl:\n","                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n","                self.optimizer.zero_grad() # clearing gradients to avoid their summation\n","\n","                with autocast(device_type=self.device): # enabling mixed precisions\n","                    pred = self.model(x_batch)\n","                    loss = self.loss_fn(pred, y_batch)\n","\n","                scaler.scale(loss).backward() # scaling loss before backpropagation serves stable performance by preventing gradient underflow in float16\n","                scaler.step(self.optimizer) # unscales gradients \n","                scaler.update() # update weights\n","\n","                total_loss_train += loss.item() * y_batch.size(0)\n","                total_correct_train += (torch.argmax(pred, dim=1) == y_batch).sum().item()\n","                total_samples_train += y_batch.size(0)\n","        \n","            train_loss = total_loss_train / len(train_dl.dataset)\n","            train_accuracy = total_correct_train / total_samples_train\n","\n","\n","            self.model.eval() # setting model to evaluation phase\n","\n","            # variables to track model performance on validation data\n","            total_correct_valid = 0\n","            total_samples_valid = 0\n","            total_loss_valid = 0\n","\n","            with torch.inference_mode(): # disabling gradients calculation to measure the model performance during epoch\n","                for x_batch, y_batch in valid_dl:\n","                    x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n","\n","                    with autocast(device_type=self.device):\n","                        pred = self.model(x_batch)\n","                        loss = self.loss_fn(pred, y_batch)\n","\n","                    total_loss_valid += loss.item() * y_batch.size(0)\n","                    total_correct_valid += (torch.argmax(pred, dim=1) == y_batch).sum().item()\n","                    total_samples_valid += y_batch.size(0)\n","\n","            valid_loss = total_loss_valid / len(valid_dl.dataset)\n","            valid_accuracy = total_correct_valid / total_samples_valid\n","\n","\n","            # writing losses and accuracies to folders to track them in tensorboard\n","            writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n","            writer.add_scalar(\"Loss/Validation\", valid_loss, epoch)\n","            writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n","            writer.add_scalar(\"Accuracy/Valid\", valid_accuracy, epoch)\n","\n","            print(f'Epoch {epoch + 1}, train acc: {train_accuracy:.4f}, valid acc: {valid_accuracy:.4f}')\n","\n","        writer.close()\n","            \n","    def predict(self, test_dl):\n","        self.model.eval()\n","        total_correct = 0\n","        total_samples = 0\n","        with torch.inference_mode():\n","            for x_batch, y_batch in test_dl:\n","                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n","                pred = self.model(x_batch)\n","                \n","                total_correct += (torch.argmax(pred, dim=1) == y_batch).sum().item()\n","                total_samples += y_batch.size(0)\n","                \n","        return (total_correct / total_samples)\n","    \n","# Random Forest implementation   \n","class RandomForestMnistClassifier(MnistClassifierInterface):\n","    def __init__(self):\n","        self.model = RandomForestClassifier()\n","        \n","    def train(self, X_train, y_train):\n","        X_train_flattened = X_train.reshape(X_train.shape[0], -1) # flatten to receive 1d array, as the model train with 2d array\n","        self.model.fit(X_train_flattened, y_train)\n","        \n","    def predict(self, X_test):\n","        X_test_flattened = X_test.reshape(X_test.shape[0], -1) # flatten to receive 1d array, as the model predict with 2d array\n","        return self.model.predict(X_test_flattened)\n","\n","# Feed-Forward Neural Network      \n","class FFNN(nn.Module):\n","    def __init__(self, input_dim=784, hidden_dim=64, output_dim=10):\n","        super().__init__() # call parent constructor to initialize arguments\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","    \n","    # forward method to pass input data thought fully connected layers and introduce non-linearity by activation function ReLU    \n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = F.relu(self.fc1(x))\n","        return self.fc2(x)\n","    \n","class FFNNMnistClassifier(TrainPredictAbstract):\n","   def __init__(self, num_epochs=10, lr=0.0001, device='cpu'):\n","       model = FFNN()\n","       super().__init__(model=model, num_epochs=num_epochs, lr=lr, device=device)\n","    \n","    \n","class CNN(nn.Module):\n","    def __init__(self, output_dim=10):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) # input_channel = 1 because image is grayscale, [batch(64), 32, 28, 28]\n","        self.batchnorm1 = nn.BatchNorm2d(32) # by normalizing layers inputs we make training faster and more stable\n","        self.pool1 = nn.MaxPool2d(kernel_size=2) # [batch, 32, 14, 14]\n","\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #  [batch, 64, 14, 14]\n","        self.batchnorm2 = nn.BatchNorm2d(64) \n","        self.pool2 = nn.MaxPool2d(kernel_size=2) # [batch, 64, 7, 7]\n","        \n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","        self.relu3 = nn.ReLU()\n","        self.drop1 = nn.Dropout(p=0.4) # disable some neurons while training to reduce overfitting\n","        self.fc2 = nn.Linear(128, output_dim)\n","        \n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.batchnorm1(self.conv1(x)))) # stack methods to reduce space \n","        \n","        x = self.pool2(F.relu(self.batchnorm2(self.conv2(x))))\n","        \n","        x = self.flatten(x)\n","        x = F.relu(self.fc1(x))\n","        x = self.drop1(x)\n","        \n","        return self.fc2(x)\n","    \n","class CNNMnistClassifier(TrainPredictAbstract):\n","    def __init__(self, num_epochs=10, lr=0.0001, device='cpu'):\n","        model = CNN()\n","        super().__init__(model=model, num_epochs=num_epochs, lr=lr, device=device)\n","        \n","        \n","class MnistClassifier:\n","    def __init__(self, algorithm='rf', **kwargs):\n","        if algorithm == 'rf':\n","            self.model = RandomForestMnistClassifier(**kwargs)\n","        elif algorithm == 'nn':\n","            self.model = FFNNMnistClassifier(**kwargs)\n","        elif algorithm == 'cnn':\n","            self.model = CNNMnistClassifier(**kwargs)\n","        else:\n","            raise ValueError(\"Incorrect alogrithm, pick one of those: 'cnn', 'nn', 'rf'\")\n","    \n","    def train(self, X_train=None, y_train=None, train_dl=None, valid_dl=None):\n","        if isinstance(self.model, RandomForestMnistClassifier):\n","            self.model.train(X_train, y_train)\n","        else:\n","            self.model.train(train_dl, valid_dl)\n","            \n","    def predict(self, X_test=None, test_dl=None):\n","        if isinstance(self.model, RandomForestMnistClassifier):\n","            return self.model.predict(X_test)\n","        else:\n","            return self.model.predict(test_dl)"]},{"cell_type":"markdown","metadata":{},"source":["# Random Forest Test"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:04:38.649278Z","iopub.status.busy":"2025-02-25T10:04:38.649061Z","iopub.status.idle":"2025-02-25T10:05:15.714466Z","shell.execute_reply":"2025-02-25T10:05:15.713770Z","shell.execute_reply.started":"2025-02-25T10:04:38.649259Z"},"trusted":true},"outputs":[],"source":["rf = MnistClassifier('rf')\n","rf.train(X_train=X_train, y_train=y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:05:15.715672Z","iopub.status.busy":"2025-02-25T10:05:15.715333Z","iopub.status.idle":"2025-02-25T10:05:16.074648Z","shell.execute_reply":"2025-02-25T10:05:16.073564Z","shell.execute_reply.started":"2025-02-25T10:05:15.715640Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random forest accuracy: 0.9694\n"]}],"source":["pred = rf.predict(X_test)\n","print(f\"Random forest accuracy: {(pred == y_test).sum() / y_test.shape[0]}\")"]},{"cell_type":"markdown","metadata":{},"source":["# FFNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:23:37.347460Z","iopub.status.busy":"2025-02-25T10:23:37.347113Z","iopub.status.idle":"2025-02-25T10:24:21.597973Z","shell.execute_reply":"2025-02-25T10:24:21.596916Z","shell.execute_reply.started":"2025-02-25T10:23:37.347422Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, train acc: 0.7473, valid acc: 0.8605\n","Epoch 2, train acc: 0.8819, valid acc: 0.8942\n","Epoch 3, train acc: 0.9007, valid acc: 0.9040\n","Epoch 4, train acc: 0.9088, valid acc: 0.9099\n","Epoch 5, train acc: 0.9146, valid acc: 0.9141\n"]}],"source":["ffnn = MnistClassifier('nn', num_epochs=5)\n","ffnn.train(train_dl=train_dl, valid_dl=valid_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:24:25.498865Z","iopub.status.busy":"2025-02-25T10:24:25.498409Z","iopub.status.idle":"2025-02-25T10:24:26.784711Z","shell.execute_reply":"2025-02-25T10:24:26.783826Z","shell.execute_reply.started":"2025-02-25T10:24:25.498828Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FFNN accuracy: 0.9179\n"]}],"source":["print(f\"FFNN accuracy: {ffnn.predict(test_dl=test_dl)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# CNN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:05:16.076043Z","iopub.status.busy":"2025-02-25T10:05:16.075677Z","iopub.status.idle":"2025-02-25T10:09:47.975362Z","shell.execute_reply":"2025-02-25T10:09:47.972976Z","shell.execute_reply.started":"2025-02-25T10:05:16.075993Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, train acc: 0.9024, valid acc: 0.9701\n","Epoch 2, train acc: 0.9680, valid acc: 0.9800\n","Epoch 3, train acc: 0.9772, valid acc: 0.9828\n","Epoch 4, train acc: 0.9814, valid acc: 0.9862\n","Epoch 5, train acc: 0.9848, valid acc: 0.9864\n"]}],"source":["cnn = MnistClassifier('cnn', num_epochs=5)\n","cnn.train(train_dl=train_dl, valid_dl=valid_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-25T10:15:23.385449Z","iopub.status.busy":"2025-02-25T10:15:23.385147Z","iopub.status.idle":"2025-02-25T10:15:26.667319Z","shell.execute_reply":"2025-02-25T10:15:26.666223Z","shell.execute_reply.started":"2025-02-25T10:15:23.385428Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CNN accuracy: 0.9875\n"]}],"source":["print(f\"CNN accuracy: {cnn.predict(test_dl=test_dl)}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"}},"nbformat":4,"nbformat_minor":4}
